{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0f6f614",
   "metadata": {},
   "source": [
    "## Data Profiling\n",
    "The first step of both the Cross Industry Process for Data Mining (CRISP-DM) and the Microsoft Team Data Science Process (TDSP), is understanding the business case for the project. In other words, what problem exists that could be solved if a value could be predicted? The next step is to source and understand data that pertains to the business case. This activity (data profiling) is aimed at identifying the shape of the dataset (i.e., the number of observations and features), the data types of its features, their meaning, the number of unique values contained within each feature, and the distribution of those values. In doing so it is typical to find the data in a raw form; i.e., the data is likely to be flawed to some extent that it would require preparatory measures before it could be used to train a machine learning model. Foremost among flaws are missing data. An analysis must be performed to decide whether to impute missing data, or to simply drop features or observations that have missing values. This decision is typically driven by quantifying how much data would remain if those missing features and/or observations were excluded. This is because any treatment applied to impute replacement values may influence all subsequent activities to understand the true distribution of values within the affected features, and to identify correlations among those features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b476ab97",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbbf27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d55729",
   "metadata": {},
   "source": [
    "#### Import the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde54964",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.getcwd(), 'Data')\n",
    "source_data_file = 'titanic.csv';\n",
    "\n",
    "data_file = os.path.join(data_dir, source_data_file)\n",
    "df = pd.read_csv(data_file, header=0, index_col=0)\n",
    "\n",
    "# Ensure the index values are: seed=1, increment=1\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dd6d77",
   "metadata": {},
   "source": [
    "### 1.0. Perform an Initial Profile\n",
    "The data being analyzed pertains to the sinking of the luxury passenger ship **Titanic**. Each observation describes an individual passenger aboard the ship at the time it sank, along with whether or not the passenger survived or perished in the disaster. The following table contains metadata describing each feature (attribute) captured to describe each passenger.\n",
    "\n",
    "#### Data Dictionary:\n",
    "<div style=\"margin-left: 0 px; margin-right: auto; width: 60%\"> \n",
    "    \n",
    "| Variable  | Description                        | Details                                        |\n",
    "| ----------|:-----------------------------------|:---------------------------------------------- |\n",
    "| survived  | Survival\t                         | 0 = No; 1 = Yes                                |\n",
    "| name      | First & Last Name                  | Given Name & Surname\t                          |\n",
    "| sex       | Gender                             | Male or Female                                 |\n",
    "| age       | Age                                | Numeric representing years                     |\n",
    "| sibsp     | Number of Siblings/Spouses Aboard\t | 1 = Spouse; >1 = # Siblings                    |\n",
    "| parch     | Number of Parents/Children Aboard\t |                                                |\n",
    "| ticket    | Ticket Number                      | Ticket number purchased                        |\n",
    "| fare      | Passenger Fare                     | The cost of the passenger's ticket             |\n",
    "| cabin     | Cabin                              | The cabin number                               |\n",
    "| embarked  | Port of Embarkation                | C = Cherbourg; Q = Queenstown; S = Southampton |\n",
    "| boat      | The lifeboat they boarded          | Lifeboat number                                |\n",
    "| body      | Body Weight                        | Numerical value (decimal)                      |\n",
    "| home.dest | Passengers home & destination      | Textual value                                  |\n",
    "\n",
    "</div>\n",
    "\n",
    "#### 1.1. Quantify the Observations and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535c4349",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape: {df.shape[0]} Observations x {df.shape[1]} Features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fe6115",
   "metadata": {},
   "source": [
    "#### 1.2. Determine which Features are Numerical and which are Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5423befb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017ea462",
   "metadata": {},
   "source": [
    "The **Target** variable *(survived)* is easily identified and can be seen to contain binary numeric values; making this a binary classification experiment. The **sex** feature also contains binary values, although they are textual. The **age** and **fare** features contain numerical values, as do the **sibsp** and **parch** features. Missing values (NaN) can be seen in both the **boat** and **body** features, and the **home.dest** feature contains brief textual descriptions. The **boat** feature contains *numerical* values, but is actually a categorical feature. The same is true regarding the **ticket** feature; however, it isn't obvious whether its values are naturally ordered, making it an *Ordinal* categorical feature, or if they have no natural ordering, making it a *Nominal* categorical variable. The **embarked** and **cabin** features are very clearly categorical data. Finally, we observe the **name** feature uniquely identifies each observation (passenger), and is therefore of no consequence to predicting whether a new (unseen) passenger would be likely to survive or perish.\n",
    "\n",
    "#### 1.3. Inspect each Feature's Data Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fb767b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53297388",
   "metadata": {},
   "source": [
    "#### 1.4. Inspect the Cardinality (number of unique values) of each Feature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ae5897",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b4911a",
   "metadata": {},
   "source": [
    "#### 1.5. Inspect each Feature's Unique Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ca7cb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unique_values = []\n",
    "for col in df.columns:\n",
    "    unique_values.append(df[col].unique())\n",
    "\n",
    "pd.DataFrame(list(zip(df.columns, unique_values)), columns=['Feature', 'Unique Values'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429093a9",
   "metadata": {},
   "source": [
    "These initial profiling techniques reveal that some features having numerical data types may in fact contain categorical data (e.g., survived, sibsp, parch). To better understand their meaning and influence it may be advantageous to convert them from their present numerical types to the **Object** type to indicate their categorical nature. What's more, it may be worthwhile to research their meaning for the sake of mapping descriptive labels to the existing numerical values. \n",
    "\n",
    "#### 1.6. Identify Duplicated Observations\n",
    "The initial profile has also revealed that, although there are 1309 observations in our datset, there are only 1307 unique values in the **name** feature. This indicates that we have a couple of duplicate observations; therefore, we may wish to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f324eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.duplicated(subset='name', keep=False) == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62557b7c",
   "metadata": {},
   "source": [
    "Here we observe that two passengers (Kate Connolly and James Kelly) have duplicate records. In each case, the first instance contains fewer missing values (NaN) so it should make sense to keep them and discard the second instances; however, we also observe conflicting data points between the duplicate records (e.g., age, ticket, fare, embarked) which also poses the question \"Which values are accurate, and which are not?\" The only way to determine which values are correct is to perform additional research, and this is a common scenerio a data scientist might encounter. To ensure the veracity of source data, it's important to maintain effective communications with business analysts, subject matter experts, data engineers, and data stewards.\n",
    "\n",
    "### 2.0. Quantify Any Missing Data\n",
    "#### 2.1. Inspect the number of missing values per feature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac341d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum().sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77436ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum().sort_values(ascending=True).plot.bar(figsize=(10,5))\n",
    "plt.ylabel('Number of Missing Values')\n",
    "plt.xlabel('Variables')\n",
    "plt.title('Quantifying Missing Data (Counts)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57cdbf2",
   "metadata": {},
   "source": [
    "#### 2.2. Inspect the Percentage of Missing Values per Feature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b28d850",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().mean().sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd6615c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().mean().sort_values(ascending=True).plot.bar(figsize=(10,5))\n",
    "plt.ylabel('Percentage of Missing Values')\n",
    "plt.xlabel('Variables')\n",
    "plt.title('Quantifying Missing Data (Percentage)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09a37c8",
   "metadata": {},
   "source": [
    "From these execises we can clearly see that 5 of the 13 features contain a significant number (or percentage) of missing values: age (0.20%), home.dest (0.43%), boat (0.63%), cabin (0.77%), body (0.91%). As the percentage of missing values exceeds 50% we may ask ourselves if enough data remains in those features to impute new values using the mean, median or mode of the remaining observations. What's more, we would have to wonder if these features would be influential even if they were complete.\n",
    "\n",
    "### 3.0. Inspect the Distribution of Values per Feature\n",
    "#### 3.1. Separate Numerical and Categorical Features\n",
    "Because numerical values often tend to be continuous while categorical values are inherently discrete, it is advantageous to separate them to make it easier to apply appropriate visualization techniques and/or feature engineering techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6245fce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = [col for col in df.columns if df.dtypes[col] != 'O']\n",
    "categorical_cols = [col for col in df.columns if col not in numerical_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb40e63",
   "metadata": {},
   "source": [
    "#### 3.2. Evaluate the Statistical Distribution of the Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a21a501",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[numerical_cols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e16f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[numerical_cols].hist(figsize=(12,12), bins=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade6af00",
   "metadata": {},
   "source": [
    "#### 3.3. Identify Any Outliers\n",
    "An outlier is a data point which is significantly different from the remaining data. \"An outlier is an observation which deviates so much from the other observations as to arouse suspicions that it was generated by a different mechanism.\" [D. Hawkins. Identification of Outliers, Chapman and Hall , 1980.]\n",
    "\n",
    "According to the IQR (inter-quantile range) proximity rule, a value is an outlier if it falls outside an upper boundary, *defined as 75th quantile + (IQR * 1.5)*, or a lower boundary, *defined as 25th quantile - (IQR * 1.5)*, where the inter-quantile range (IQR) is defined as *(75th quantile - 25th quantile)*.\n",
    "\n",
    "In the boxplots displayed below, the **IQR** is indicated by the (inner) box, the **median** is indicated by the line within the box, the top and bottom edges of the box correspond to the 75th and 25th quantile, and the whiskers mark the **proximity rule boundaries** as described above. Values that fall outside the whiskers are considered outliers; however, further research may be required to determine if these values are in fact erroneous, or if they represent the general truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4527bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols.remove('survived')\n",
    "for col in numerical_cols:\n",
    "    plt.figure(figsize=(10,3))\n",
    "    sns.boxplot(x=df[col])\n",
    "    plt.title(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571d5fe7",
   "metadata": {},
   "source": [
    "#### 3.4. Evaluate the Distribution of Categorical Features\n",
    "The distribution of categorical features is determined by comparing the count of each category relative to the whole. Customary visualizations for this task are the Bar Chart and the Frequency Table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc144b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[categorical_cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42a41df",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df[['survived','sex','cabin','embarked','boat']]\n",
    "\n",
    "for col in cols:\n",
    "    plt.figure(figsize=(10,4))\n",
    "    sns.countplot(x=col, data=df)\n",
    "    plt.title(col)\n",
    "    plt.xlabel(\"Count of each {0}\".format(col))\n",
    "    plt.ylabel(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae65aa19",
   "metadata": {},
   "source": [
    "#### 3.5. Identify any Rare Categories in Categorical Features\n",
    "While some categories (labels) appear frequently in categorical features, others may occur less often. In fact, it is quite typical for one or more categories to appear in a large percentage of the observations, while the remaining categories appear in a very small percentage. If a value appears very infrequently (e.g., less than 5%) then it may be considered *rare*. The only features that may contain *rare* categories are **cabin** and **boat**, so this exercise will focus on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d121976",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in ['cabin','boat']:\n",
    "    survived = pd.crosstab(index=df[col], columns=[\"Percent\"], colnames=[\"\"])\n",
    "    survived = survived.sort_values(by=['Percent'], ascending=True)\n",
    "    display(round(survived/survived.sum(), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2d35f2",
   "metadata": {},
   "source": [
    "##### View Rare Occurances by Count\n",
    "In some cases it may be more intuitive to view occurances by *count* rather than by *percentage*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fea857a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['cabin','boat']:\n",
    "    survived = pd.crosstab(index=df[col], columns=[\"Percent\"], colnames=[\"\"])\n",
    "    survived = survived.sort_values(by=['Percent'], ascending=True)\n",
    "    display(survived)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
